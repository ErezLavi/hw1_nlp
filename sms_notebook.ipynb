{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spam SMS data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset info: https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\erez1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy\n",
    "from collections import Counter\n",
    "#TODO: install pytorch?\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "5  spam  FreeMsg Hey there darling it's been 3 week's n...        NaN   \n",
       "6   ham  Even my brother is not like to speak with me. ...        NaN   \n",
       "7   ham  As per your request 'Melle Melle (Oru Minnamin...        NaN   \n",
       "8  spam  WINNER!! As a valued network customer you have...        NaN   \n",
       "9  spam  Had your mobile 11 months or more? U R entitle...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  \n",
       "5        NaN        NaN  \n",
       "6        NaN        NaN  \n",
       "7        NaN        NaN  \n",
       "8        NaN        NaN  \n",
       "9        NaN        NaN  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "spam_data = pd.read_csv('spam.csv', encoding='latin-1')\n",
    "spam_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "spam_data = spam_data[['v1', 'v2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a total of 5572 messages\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   v1                                                 v2\n",
       "0   0  Go until jurong point, crazy.. Available only ...\n",
       "1   0                      Ok lar... Joking wif u oni...\n",
       "2   1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   0  U dun say so early hor... U c already then say...\n",
       "4   0  Nah I don't think he goes to usf, he lives aro...\n",
       "5   1  FreeMsg Hey there darling it's been 3 week's n...\n",
       "6   0  Even my brother is not like to speak with me. ...\n",
       "7   0  As per your request 'Melle Melle (Oru Minnamin...\n",
       "8   1  WINNER!! As a valued network customer you have...\n",
       "9   1  Had your mobile 11 months or more? U R entitle..."
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of sms messages\n",
    "num_messages = spam_data.shape[0]\n",
    "print(\"We have a total of {} messages\".format(num_messages))\n",
    "# map ham to 0 and spam to 1\n",
    "spam_data['v1'] = spam_data['v1'].map({'ham': 0, 'spam': 1})\n",
    "spam_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a total of 747 spam messages\n",
      "We have a total of 86961 words\n",
      "On average, we have 15.60678391959799 words per message\n"
     ]
    }
   ],
   "source": [
    "# Compute number of spam messages\n",
    "num_spam = spam_data['v1'].sum()\n",
    "print(\"We have a total of {} spam messages\".format(num_spam))\n",
    "messages = spam_data['v2'].values\n",
    "num_of_words = sum([len(message.split(\" \")) for message in messages])\n",
    "print(\"We have a total of {} words\".format(num_of_words))\n",
    "avg_num_words = num_of_words / num_messages\n",
    "print(\"On average, we have {} words per message\".format(avg_num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 most common words:\n",
      "[('to', 2134), ('you', 1622), ('I', 1466), ('a', 1327), ('the', 1197)]\n",
      "Number of words that appear once: 9270\n"
     ]
    }
   ],
   "source": [
    "# all words counter\n",
    "words_counter = {}\n",
    "for message in messages:\n",
    "    for word in message.split(\" \"):\n",
    "        if word not in words_counter:\n",
    "            words_counter[word] = 1\n",
    "        else:\n",
    "            words_counter[word] += 1\n",
    "# print 5 most common words\n",
    "print(\"5 most common words:\")\n",
    "print(sorted(words_counter.items(), key=lambda x: x[1], reverse=True)[:5])\n",
    "\n",
    "appear_once_words = [word for word, count in words_counter.items() if count == 1]\n",
    "print(\"Number of words that appear once: {}\".format(len(appear_once_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization: NLTK VS Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Go', 'until', 'jurong', 'point', ',', 'crazy', '..', 'Available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', '...', 'Cine', 'there', 'got', 'amore', 'wat', '...', 'Ok', 'lar', '...', 'Joking', 'wif', 'u', 'oni', '...', 'Free', 'entry', 'in', '2', 'a', 'wkly', 'comp', 'to', 'win', 'FA', 'Cup', 'final', 'tkts', '21st', 'May', '2005', '.', 'Text', 'FA', 'to', '87121', 'to', 'receive', 'entry', 'question', '(', 'std', 'txt', 'rate', ')', 'T', '&', 'C', \"'s\", 'apply', '08452810075over18', \"'s\", 'U', 'dun', 'say', 'so', 'early', 'hor', '...', 'U', 'c', 'already', 'then', 'say', '...', 'Nah', 'I', 'do', \"n't\", 'think', 'he', 'goes', 'to', 'usf', ',', 'he', 'lives', 'around', 'here', 'though', 'FreeMsg', 'Hey', 'there', 'darling', 'it', \"'s\", 'been', '3', 'week', \"'s\", 'now', 'and', 'no', 'word', 'back', '!', 'I', \"'d\", 'like', 'some', 'fun', 'you', 'up']\n"
     ]
    }
   ],
   "source": [
    "all_messages = \" \".join(messages)\n",
    "# perform tokenization using nltk\n",
    "res = nltk.word_tokenize(all_messages)\n",
    "print(res[0:120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Go, until, jurong, point,, crazy.., Available, only, in, bugis, n, great, world, la, e, buffet..., Cine, there, got, amore, wat..., Ok, lar..., Joking, wif, u, oni..., Free, entry, in, 2, a, wkly, comp, to, win, FA, Cup, final, tkts, 21st, May, 2005., Text, FA, to, 87121, to, receive, entry, question(std, txt, rate)T&C's, apply, 08452810075over18's, U, dun, say, so, early, hor..., U, c, already, then, say..., Nah, I, don't, think, he, goes, to, usf,, he, lives, around, here, though, FreeMsg, Hey, there, darling, it's, been, 3, week's, now, and, no, word, back!, I'd, like, some, fun, you, up, for, it, still?, Tb, ok!, XxX, std, chgs, to, send,, å£1.50, to, rcv, Even, my, brother, is, not, like, to, speak, with, me.]\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "tokens = tokenizer(all_messages)\n",
    "print(list(tokens)[0:120])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "The main differences we have spotted are:\n",
    "1. nltk tokenizer takes punctuation as separated tokens, while spacy's tokenizer concat them to the nearest word\n",
    "2. spacy's tokenizer chose to remain words with ' like \"don't\" as a whole token, while nltk tokenizer separated it to 'do' and \"n't\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lemmatization: NLTK VS Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Go', 'until', 'jurong', 'point', ',', 'crazy', '..', 'Available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', '...', 'Cine', 'there', 'got', 'amore', 'wat', '...', 'Ok', 'lar', '...', 'Joking', 'wif', 'u', 'oni', '...', 'Free', 'entry', 'in', '2', 'a', 'wkly', 'comp', 'to', 'win', 'FA', 'Cup', 'final', 'tkts', '21st', 'May', '2005', '.', 'Text', 'FA', 'to', '87121', 'to', 'receive', 'entry', 'question', '(', 'std', 'txt', 'rate', ')', 'T', '&', 'C', \"'s\", 'apply', '08452810075over18', \"'s\", 'U', 'dun', 'say', 'so', 'early', 'hor', '...', 'U', 'c', 'already', 'then', 'say', '...', 'Nah', 'I', 'do', \"n't\", 'think', 'he', 'go', 'to', 'usf', ',', 'he', 'life', 'around', 'here', 'though', 'FreeMsg', 'Hey', 'there', 'darling', 'it', \"'s\", 'been', '3', 'week', \"'s\", 'now', 'and', 'no', 'word', 'back', '!', 'I', \"'d\", 'like', 'some', 'fun', 'you', 'up']\n"
     ]
    }
   ],
   "source": [
    "# perform lemmatization using nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words_nltk = [lemmatizer.lemmatize(token) for token in res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['go', 'until', 'jurong', 'point', ',', 'crazy', '..', 'available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', '...', 'Cine', 'there', 'get', 'amore', 'wat', '...', 'ok', 'lar', '...', 'joke', 'wif', 'u', 'oni', '...', 'free', 'entry', 'in', '2', 'a', 'wkly', 'comp', 'to', 'win', 'FA', 'Cup', 'final', 'tkts', '21st', 'May', '2005', '.', 'text', 'fa', 'to', '87121', 'to', 'receive', 'entry', 'question(std', 'txt', 'rate)T&C', \"'s\", 'apply', '08452810075over18', \"'s\", 'u', 'dun', 'say', 'so', 'early', 'hor', '...', 'u', 'c', 'already', 'then', 'say', '...', 'Nah', 'I', 'do', 'not', 'think', 'he', 'go', 'to', 'usf', ',', 'he', 'live', 'around', 'here', 'though', 'FreeMsg', 'hey', 'there', 'darle', 'it', 'be', 'be', '3', 'week', \"'s\", 'now', 'and', 'no', 'word', 'back', '!', 'I', 'would', 'like', 'some', 'fun', 'you', 'up', 'for', 'it', 'still', '?', 'tb', 'ok']\n"
     ]
    }
   ],
   "source": [
    "# perform lemmatization using spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "lemmatized_words_spacy = [\n",
    "    token.lemma_\n",
    "    for message in messages\n",
    "    for token in nlp(message)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK lemmatized words:\n",
      "['Go', 'until', 'jurong', 'point', ',', 'crazy', '..', 'Available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', '...', 'Cine', 'there', 'got', 'amore', 'wat', '...', 'Ok', 'lar', '...', 'Joking', 'wif', 'u', 'oni', '...', 'Free', 'entry', 'in', '2', 'a', 'wkly', 'comp', 'to', 'win', 'FA', 'Cup', 'final', 'tkts', '21st', 'May', '2005', '.', 'Text', 'FA', 'to', '87121', 'to', 'receive', 'entry', 'question', '(', 'std', 'txt', 'rate', ')', 'T', '&', 'C', \"'s\", 'apply', '08452810075over18', \"'s\", 'U', 'dun', 'say', 'so', 'early', 'hor', '...', 'U', 'c', 'already', 'then', 'say', '...', 'Nah', 'I', 'do', \"n't\", 'think', 'he', 'go', 'to', 'usf', ',', 'he', 'life', 'around', 'here', 'though', 'FreeMsg', 'Hey', 'there', 'darling', 'it', \"'s\", 'been', '3', 'week', \"'s\", 'now', 'and', 'no', 'word', 'back', '!', 'I', \"'d\", 'like', 'some', 'fun', 'you', 'up']\n",
      "Length of NLTK lemmatized words:\n",
      "104164\n",
      "Spacy lemmatized words:\n",
      "['go', 'until', 'jurong', 'point', ',', 'crazy', '..', 'available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', '...', 'Cine', 'there', 'get', 'amore', 'wat', '...', 'ok', 'lar', '...', 'joke', 'wif', 'u', 'oni', '...', 'free', 'entry', 'in', '2', 'a', 'wkly', 'comp', 'to', 'win', 'FA', 'Cup', 'final', 'tkts', '21st', 'May', '2005', '.', 'text', 'fa', 'to', '87121', 'to', 'receive', 'entry', 'question(std', 'txt', 'rate)T&C', \"'s\", 'apply', '08452810075over18', \"'s\", 'u', 'dun', 'say', 'so', 'early', 'hor', '...', 'u', 'c', 'already', 'then', 'say', '...', 'Nah', 'I', 'do', 'not', 'think', 'he', 'go', 'to', 'usf', ',', 'he', 'live', 'around', 'here', 'though', 'FreeMsg', 'hey', 'there', 'darle', 'it', 'be', 'be', '3', 'week', \"'s\", 'now', 'and', 'no', 'word', 'back', '!', 'I', 'would', 'like', 'some', 'fun', 'you', 'up', 'for', 'it', 'still', '?', 'tb', 'ok']\n",
      "Length of Spacy lemmatized words:\n",
      "103533\n"
     ]
    }
   ],
   "source": [
    "print(\"NLTK lemmatized words:\")\n",
    "print(lemmatized_words_nltk[:120])\n",
    "print(\"Length of NLTK lemmatized words:\")\n",
    "print(len(lemmatized_words_nltk))\n",
    "print(\"Spacy lemmatized words:\")\n",
    "print(lemmatized_words_spacy[:120])\n",
    "print(\"Length of Spacy lemmatized words:\")\n",
    "print(len(lemmatized_words_spacy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "The main differnces are: \n",
    "1. Running time: Spacy is much slower due to running ML models in order to return a more accurate results. For instance, the POS tagging, which predicts the grammatical role of each word (noun, verb, etc..)\n",
    "2. nltk lemmatizer returned words like \"it's\" while spacy splitted it to \"it\", \"be\" ('s -> is -> be).\n",
    "3. nltk's lemmatizer didn't lemmatize the work 'joking' (stayed the same) while spacy's lemmatizer change it to 'joke'. \n",
    "4. nltk's returned the 'me' while spacy returned 'i'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming: NLTK VS Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = nltk.LancasterStemmer()\n",
    "# nltk stemming\n",
    "stemmed_words_nltk = [ls.stem(word) for word in lemmatized_words_nltk]\n",
    "#spacy stemming\n",
    "stemmed_words_spacy = [ls.stem(word) for word in lemmatized_words_spacy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK stemming\n",
      "['go', 'until', 'jurong', 'point', ',', 'crazy', '..', 'avail', 'on', 'in', 'bug', 'n', 'gre', 'world', 'la', 'e', 'buffet', '...', 'cin', 'ther', 'got', 'am', 'wat', '...', 'ok', 'lar', '...', 'jok', 'wif', 'u', 'on', '...', 'fre', 'entry', 'in', '2', 'a', 'wkly', 'comp', 'to', 'win', 'fa', 'cup', 'fin', 'tkts', '21st', 'may', '2005', '.', 'text', 'fa', 'to', '87121', 'to', 'receiv', 'entry', 'quest', '(', 'std', 'txt', 'rat', ')', 't', '&', 'c', \"'s\", 'apply', '08452810075over18', \"'s\", 'u', 'dun', 'say', 'so', 'ear', 'hor', '...', 'u', 'c', 'already', 'then', 'say', '...', 'nah', 'i', 'do', \"n't\", 'think', 'he', 'go', 'to', 'usf', ',', 'he', 'lif', 'around', 'her', 'though', 'freemsg', 'hey', 'ther', 'darl', 'it', \"'s\", 'been', '3', 'week', \"'s\", 'now', 'and', 'no', 'word', 'back', '!', 'i', \"'d\", 'lik', 'som', 'fun', 'you', 'up']\n",
      "Length of NLTK stemmed words: 104164\n",
      "Spacy stemming\n",
      "['go', 'until', 'jurong', 'point', ',', 'crazy', '..', 'avail', 'on', 'in', 'bug', 'n', 'gre', 'world', 'la', 'e', 'buffet', '...', 'cin', 'ther', 'get', 'am', 'wat', '...', 'ok', 'lar', '...', 'jok', 'wif', 'u', 'on', '...', 'fre', 'entry', 'in', '2', 'a', 'wkly', 'comp', 'to', 'win', 'fa', 'cup', 'fin', 'tkts', '21st', 'may', '2005', '.', 'text', 'fa', 'to', '87121', 'to', 'receiv', 'entry', 'question(std', 'txt', 'rate)t&c', \"'s\", 'apply', '08452810075over18', \"'s\", 'u', 'dun', 'say', 'so', 'ear', 'hor', '...', 'u', 'c', 'already', 'then', 'say', '...', 'nah', 'i', 'do', 'not', 'think', 'he', 'go', 'to', 'usf', ',', 'he', 'liv', 'around', 'her', 'though', 'freemsg', 'hey', 'ther', 'darl', 'it', 'be', 'be', '3', 'week', \"'s\", 'now', 'and', 'no', 'word', 'back', '!', 'i', 'would', 'lik', 'som', 'fun', 'you', 'up', 'for', 'it', 'stil', '?', 'tb', 'ok']\n",
      "Length of Spacy stemmed words: 103533\n"
     ]
    }
   ],
   "source": [
    "print(\"NLTK stemming\")\n",
    "print(stemmed_words_nltk[:120])\n",
    "print(\"Length of NLTK stemmed words: {}\".format(len(stemmed_words_nltk)))\n",
    "print(\"Spacy stemming\")\n",
    "print(stemmed_words_spacy[:120])\n",
    "print(\"Length of Spacy stemmed words: {}\".format(len(stemmed_words_spacy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "There is no significant differences between the two results. \n",
    "We have noticed that the nltk's LancasterStemmer has lower cased all word's first letter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "As we can see from out results, the length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized message:\n",
      "Sunshine Quiz Wkly Q! Win a top Sony DVD player if u know which country the Algarve is in? Txt ansr to 82277. å£1.50 SP:Tyrone\n",
      "Stemmed message:\n",
      "sunshine quiz wkly q! win a top sony dvd player if u know which country the algarve is in? txt ansr to 82277. å£1.50 sp:tyron\n",
      "The stemmed message length is: 125\n",
      "The lemmatized message length is: 126\n",
      "Lemmatized message:\n",
      "UpgrdCentre Orange customer, you may now claim your FREE CAMERA PHONE upgrade for your loyalty. Call now on 0207 153 9153. Offer ends 26th July. T&C's apply. Opt-out available\n",
      "Stemmed message:\n",
      "upgrdcentre orange customer, you may now claim your free camera phone upgrade for your loyalty. call now on 0207 153 9153. offer ends 26th july. t&c's apply. opt-out availabl\n",
      "The stemmed message length is: 174\n",
      "The lemmatized message length is: 175\n",
      "Lemmatized message:\n",
      "Boltblue tones for 150p Reply POLY# or MONO# eg POLY3 1. Cha Cha Slide 2. Yeah 3. Slow Jamz 6. Toxic 8. Come With Me or STOP 4 more tones txt MORE\n",
      "Stemmed message:\n",
      "boltblue tones for 150p reply poly# or mono# eg poly3 1. cha cha slide 2. yeah 3. slow jamz 6. toxic 8. come with me or stop 4 more tones txt mor\n",
      "The stemmed message length is: 145\n",
      "The lemmatized message length is: 146\n",
      "Lemmatized message:\n",
      "FREE for 1st week! No1 Nokia tone 4 ur mobile every week just txt NOKIA to 8077 Get txting and tell ur mates. www.getzed.co.uk POBox 36504 W45WQ 16+ norm150p/tone\n",
      "Stemmed message:\n",
      "free for 1st week! no1 nokia tone 4 ur mobile every week just txt nokia to 8077 get txting and tell ur mates. www.getzed.co.uk pobox 36504 w45wq 16+ norm150p/ton\n",
      "The stemmed message length is: 161\n",
      "The lemmatized message length is: 162\n",
      "Lemmatized message:\n",
      "Congratulations ur awarded either a yrs supply of CDs from Virgin Records or a Mystery Gift GUARANTEED Call 09061104283 Ts&Cs www.smsco.net å£1.50pm approx 3mins\n",
      "Stemmed message:\n",
      "congratulations ur awarded either a yrs supply of cds from virgin records or a mystery gift guaranteed call 09061104283 ts&cs www.smsco.net å£1.50pm approx 3min\n",
      "The stemmed message length is: 160\n",
      "The lemmatized message length is: 161\n",
      "Lemmatized message:\n",
      "Sunshine Quiz Wkly Q! Win a top Sony DVD player if u know which country Liverpool played in mid week? Txt ansr to 82277. å£1.50 SP:Tyrone\n",
      "Stemmed message:\n",
      "sunshine quiz wkly q! win a top sony dvd player if u know which country liverpool played in mid week? txt ansr to 82277. å£1.50 sp:tyron\n",
      "The stemmed message length is: 136\n",
      "The lemmatized message length is: 137\n",
      "Lemmatized message:\n",
      "As a registered optin subscriber ur draw 4 å£100 gift voucher will be entered on receipt of a correct ans to 80062 Whats No1 in the BBC charts\n",
      "Stemmed message:\n",
      "as a registered optin subscriber ur draw 4 å£100 gift voucher will be entered on receipt of a correct ans to 80062 whats no1 in the bbc chart\n",
      "The stemmed message length is: 141\n",
      "The lemmatized message length is: 142\n",
      "Lemmatized message:\n",
      "Sunshine Quiz Wkly Q! Win a top Sony DVD player if u know which country the Algarve is in? Txt ansr to 82277. å£1.50 SP:Tyrone\n",
      "Stemmed message:\n",
      "sunshine quiz wkly q! win a top sony dvd player if u know which country the algarve is in? txt ansr to 82277. å£1.50 sp:tyron\n",
      "The stemmed message length is: 125\n",
      "The lemmatized message length is: 126\n",
      "Lemmatized message:\n",
      "Sunshine Quiz Wkly Q! Win a top Sony DVD player if u know which country Liverpool played in mid week? Txt ansr to 82277. å£1.50 SP:Tyrone\n",
      "Stemmed message:\n",
      "sunshine quiz wkly q! win a top sony dvd player if u know which country liverpool played in mid week? txt ansr to 82277. å£1.50 sp:tyron\n",
      "The stemmed message length is: 136\n",
      "The lemmatized message length is: 137\n",
      "Lemmatized message:\n",
      "Burger King - Wanna play footy at a top stadium? Get 2 Burger King before 1st Sept and go Large or Super with Coca-Cola and walk out a winner\n",
      "Stemmed message:\n",
      "burger king - wanna play footy at a top stadium? get 2 burger king before 1st sept and go large or super with coca-cola and walk out a winn\n",
      "The stemmed message length is: 139\n",
      "The lemmatized message length is: 141\n",
      "Lemmatized message:\n",
      "FREE UNLIMITED HARDCORE PORN direct 2 your mobile Txt PORN to 69200 & get FREE access for 24 hrs then chrgd@50p per day txt Stop 2exit. This msg is free\n",
      "Stemmed message:\n",
      "free unlimited hardcore porn direct 2 your mobile txt porn to 69200 & get free access for 24 hrs then chrgd@50p per day txt stop 2exit. this msg is fr\n",
      "The stemmed message length is: 150\n",
      "The lemmatized message length is: 152\n",
      "Lemmatized message:\n",
      "FREE MSG:We billed your mobile number by mistake from shortcode 83332.Please call 08081263000 to have charges refunded.This call will be free from a BT landline\n",
      "Stemmed message:\n",
      "free msg:we billed your mobile number by mistake from shortcode 83332.please call 08081263000 to have charges refunded.this call will be free from a bt landlin\n",
      "The stemmed message length is: 159\n",
      "The lemmatized message length is: 160\n",
      "Lemmatized message:\n",
      "FREE for 1st week! No1 Nokia tone 4 ur mobile every week just txt NOKIA to 8077 Get txting and tell ur mates. www.getzed.co.uk POBox 36504 W45WQ 16+ norm150p/tone\n",
      "Stemmed message:\n",
      "free for 1st week! no1 nokia tone 4 ur mobile every week just txt nokia to 8077 get txting and tell ur mates. www.getzed.co.uk pobox 36504 w45wq 16+ norm150p/ton\n",
      "The stemmed message length is: 161\n",
      "The lemmatized message length is: 162\n",
      "Number of spam messages where the number of lemmatized tokens is different from the stemmed tokens: 13\n"
     ]
    }
   ],
   "source": [
    "# Identify a spam message where its removal from the spam.csv dataset would:\n",
    "# a) Reduce the total number of stemmed tokens\n",
    "# b) Maintain the exact same number of lemmatized tokens\n",
    "count = 0\n",
    "spam_messages = spam_data[spam_data['v1'] == 1]['v2'].values\n",
    "for message in spam_messages:\n",
    "    stem_message = ls.stem(message)\n",
    "    stem_len = len(stem_message)\n",
    "    llematized_message = lemmatizer.lemmatize(message)\n",
    "    llem_len = len(llematized_message)\n",
    "    if(llem_len != stem_len):\n",
    "        count += 1\n",
    "        print(\"Lemmatized message:\")\n",
    "        print(llematized_message)\n",
    "        print(\"Stemmed message:\")\n",
    "        print(stem_message)\n",
    "        print(\"The stemmed message length is: {}\".format(stem_len))\n",
    "        print(\"The lemmatized message length is: {}\".format(llem_len))\n",
    "\n",
    "print(\"Number of spam messages where the number of lemmatized tokens is different from the stemmed tokens: {}\".format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over all the mesages if the message isnt spam - than add all the tokens of llematized and stemming that didnt appear in the arrays(stemming tokens and llematized tokens)\n",
    "# if the message is spam - than check if all tokens are in the stemming and lemmatized arrays"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw1_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
